{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running Foundation Model CleanTS on gift-eval benchmark\n",
    "\n",
    "This notebook presents the reproduction code of CleanTS for Gift-eval."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Install CleanTS\n",
    "We first install CleanTS via\n",
    "`pip install git+https://github.com/Taihuachen-cfair/CleanTS.git`.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run eval with CleanTS"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# skip the no mean value warning from QuantileForecast\n",
    "logging.getLogger(\"gluonts\").setLevel(logging.ERROR)\n",
    "\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# 配置项\n",
    "# 数据集信息\n",
    "short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "dataset_properties_map_path= \"dataset_properties.json\"\n",
    "# json.load(open(\"dataset_properties.json\"))\n",
    "# 评估配置\n",
    "max_batch_size=512\n",
    "context_length=3000\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "dataset_properties_map = json.load(open(dataset_properties_map_path))\n",
    "\n",
    "# 评估尺度实例化\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]\n",
    "from uni2ts.model.CleanTS import CleanTSForecast, CleanTSModule\n",
    "model = CleanTSForecast(\n",
    "    module=CleanTSModule.from_pretrained(\"EINK/CleanTS-65M\"),\n",
    "    prediction_length=1,\n",
    "    context_length=context_length,\n",
    "    feat_dynamic_real_dim=0,\n",
    "    past_feat_dynamic_real_dim=0,\n",
    ")\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "import csv\n",
    "import os\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "# Iterate over all available datasets\n",
    "\n",
    "output_dir = \"CleanTS/results\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "header = [\n",
    "    \"dataset\",\n",
    "    \"model_small\",\n",
    "    \"eval_metrics/MSE[mean]\",\n",
    "    \"eval_metrics/MSE[0.5]\",\n",
    "    \"eval_metrics/MAE[0.5]\",\n",
    "    \"eval_metrics/MASE[0.5]\",\n",
    "    \"eval_metrics/MAPE[0.5]\",\n",
    "    \"eval_metrics/sMAPE[0.5]\",\n",
    "    \"eval_metrics/MSIS\",\n",
    "    \"eval_metrics/RMSE[mean]\",\n",
    "    \"eval_metrics/NRMSE[mean]\",\n",
    "    \"eval_metrics/ND[0.5]\",\n",
    "    \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "    \"domain\",\n",
    "    \"num_variates\",\n",
    "]\n",
    "results=[]\n",
    "for ds_name in all_datasets:\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name}\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        to_univariate = False if Dataset(name=ds_name, term=term,to_univariate=False).target_dim == 1 else True\n",
    "\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "\n",
    "        # set the hyperparameter according to each dataset, then create the predictor\n",
    "        model.hparams.prediction_length = dataset.prediction_length\n",
    "        model.hparams.target_dim = dataset.target_dim\n",
    "        model.hparams.past_feat_dynamic_real_dim = dataset.past_feat_dynamic_real_dim\n",
    "\n",
    "        batch_size = max_batch_size\n",
    "        while batch_size >= 1:\n",
    "            try:\n",
    "                predictor = model.create_predictor(batch_size=batch_size)\n",
    "                season_length = get_seasonality(dataset.freq)\n",
    "                res = evaluate_model(\n",
    "                    predictor,\n",
    "                    test_data=dataset.test_data,\n",
    "                    metrics=metrics,\n",
    "                    axis=None,\n",
    "                    mask_invalid_label=True,\n",
    "                    allow_nan_forecast=False,\n",
    "                    seasonality=season_length,\n",
    "                )\n",
    "                break  # 成功则跳出循环\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e) or \"out of memory\" in str(e).lower():\n",
    "                    print(f\"CUDA out of memory with batch_size={batch_size}, halving...\")\n",
    "                    batch_size //= 2\n",
    "                    if batch_size < 1:\n",
    "                        raise RuntimeError(\"Batch size reduced below 1; cannot proceed.\") from e\n",
    "                    import torch\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                else:\n",
    "                    raise\n",
    "        else:\n",
    "            raise RuntimeError(\"Evaluation failed even with batch_size=1\")\n",
    "\n",
    "        result_row = [\n",
    "            ds_config,\n",
    "            \"CleanTS-65M\",\n",
    "            res[\"MSE[mean]\"][0],\n",
    "            res[\"MSE[0.5]\"][0],\n",
    "            res[\"MAE[0.5]\"][0],\n",
    "            res[\"MASE[0.5]\"][0],\n",
    "            res[\"MAPE[0.5]\"][0],\n",
    "            res[\"sMAPE[0.5]\"][0],\n",
    "            res[\"MSIS\"][0],\n",
    "            res[\"RMSE[mean]\"][0],\n",
    "            res[\"NRMSE[mean]\"][0],\n",
    "            res[\"ND[0.5]\"][0],\n",
    "            res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "            dataset_properties_map[ds_key][\"domain\"],\n",
    "            dataset_properties_map[ds_key][\"num_variates\"],\n",
    "        ]\n",
    "\n",
    "        results.append((ds_config, result_row))\n",
    "\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for _, row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Results written to {csv_file_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gift_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
