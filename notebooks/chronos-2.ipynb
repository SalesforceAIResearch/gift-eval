{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b969a3",
   "metadata": {},
   "source": [
    "# Running Chronos 2 on gift-eval benchmark\n",
    "\n",
    "**The following notebook is only intended to reproduce GIFT-Eval results using a GluonTS-style predictor interface. For practical usage, we recommend using the simpler interface of Chronos-2 as described in the [Github repo](https://github.com/amazon-science/chronos-forecasting).**\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `SHORT_DATASETS` and `MED_LONG_DATASETS` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8ff90",
   "metadata": {},
   "source": [
    "Install Chronos package:\n",
    "``\n",
    "pip install chronos-forecasting>=2.0\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#SHORT_DATASETS = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "SHORT_DATASETS = \"m4_weekly\"\n",
    "\n",
    "#MED_LONG_DATASETS = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "MED_LONG_DATASETS = \"bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(SHORT_DATASETS.split() + MED_LONG_DATASETS.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))\n",
    "\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline, Chronos2Pipeline\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"Chronos-2 Predictor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Chronos2Predictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        prediction_length: int,\n",
    "        batch_size: int,\n",
    "        quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        predict_batches_jointly: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            model_name,\n",
    "            **kwargs,\n",
    "        )\n",
    "        assert isinstance(self.pipeline, Chronos2Pipeline), \"This is Predictor is for Chronos-2, see other notebook for Chronos and Chronos-Bolt\"\n",
    "        self.prediction_length = prediction_length\n",
    "        self.batch_size = batch_size\n",
    "        self.quantile_levels = quantile_levels\n",
    "        self.predict_batches_jointly = predict_batches_jointly\n",
    "\n",
    "\n",
    "    def _pack_model_items(self, items):\n",
    "        for item in items:\n",
    "            model_input = {\n",
    "                \"target\": item[\"target\"],\n",
    "            }\n",
    "            yield model_input\n",
    "\n",
    "\n",
    "    def predict(self, test_data_input) -> List[Forecast]:\n",
    "        pipeline = self.pipeline\n",
    "        model_batch_size = self.batch_size\n",
    "        if self.predict_batches_jointly:\n",
    "            logger.info(\"Note: Using cross learning mode. Please ensure that different rolling windows of the same time series are not in `test_data_input` to avoid any potential leakage due to in-context learning.\")\n",
    "\n",
    "        # Generate forecasts\n",
    "        forecast_outputs = []\n",
    "        input_data = list(self._pack_model_items(test_data_input))\n",
    "        is_univariate_data = input_data[0][\"target\"].ndim == 1  # homogenous across all intputs\n",
    "        while True:\n",
    "            try:\n",
    "                quantiles, _ = pipeline.predict_quantiles(\n",
    "                        inputs=input_data,\n",
    "                        prediction_length=self.prediction_length,\n",
    "                        batch_size=model_batch_size,\n",
    "                        quantile_levels=self.quantile_levels,\n",
    "                        predict_batches_jointly=self.predict_batches_jointly,\n",
    "                )\n",
    "                quantiles = torch.stack(quantiles)\n",
    "                # quantiles [batch, variates, seq_len, quantiles]\n",
    "                quantiles = quantiles.permute(0, 3, 2, 1).cpu().numpy()\n",
    "                # forecast_outputs [batch, quantiles, seq_len, variates]\n",
    "                if is_univariate_data:\n",
    "                    quantiles = quantiles.squeeze(-1) # squeeze variate to avoid error in eval due to broadcasting\n",
    "                assert quantiles.shape[1] == len(self.quantile_levels)\n",
    "                assert quantiles.shape[2] == self.prediction_length\n",
    "                forecast_outputs.append(quantiles)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                logger.error(f\"OutOfMemoryError at model_batch_size {model_batch_size}, reducing to {model_batch_size // 2}\")\n",
    "                model_batch_size //= 2\n",
    "\n",
    "        # Convert forecasts into gluonts Forecast objects\n",
    "        forecast_outputs = np.concatenate(forecast_outputs, axis=0)\n",
    "        assert len(forecast_outputs) == len(input_data)\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "            forecast = QuantileForecast(\n",
    "                forecast_arrays=item,\n",
    "                forecast_keys=list(map(str, self.quantile_levels)),\n",
    "                start_date=forecast_start_date,\n",
    "            )\n",
    "            forecasts.append(forecast)\n",
    "        return forecasts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9d567",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/chronos-2` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1567ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0aeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fe5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from gluonts.model import evaluate_forecasts\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "model_name = \"s3://autogluon/chronos-2\"\n",
    "output_dir = f\"../results/chronos-2/all_results.csv\"\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "pretty_model_name = {\n",
    "    \"s3://autogluon/chronos-2\": \"Chronos-2\",\n",
    "}\n",
    "\n",
    "def evaluate_on_dataset(\n",
    "    model_name: str,\n",
    "    ds_name: str,\n",
    "    ds_term: str,\n",
    "    batch_size: int,\n",
    "    use_multivariate_data: bool = True,\n",
    "    **predictor_kwargs,\n",
    "):\n",
    "    is_multivariate_source = (\n",
    "        Dataset(\n",
    "            name=ds_name,\n",
    "            term=ds_term,\n",
    "            to_univariate=False,\n",
    "        ).target_dim\n",
    "        > 1\n",
    "    )\n",
    "\n",
    "    dataset = Dataset(\n",
    "        name=ds_name,\n",
    "        term=ds_term,\n",
    "        to_univariate=is_multivariate_source and not use_multivariate_data,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "    predictor = Chronos2Predictor(\n",
    "        model_name=model_name,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        batch_size=batch_size,\n",
    "        **predictor_kwargs,\n",
    "    )\n",
    "    \n",
    "    # Avoid cross batch leakage of rolling evalution by prediction of windows individually.\n",
    "    forecast_windows = []\n",
    "    n_windows = dataset.test_data.windows\n",
    "    for window_idx in range(n_windows):\n",
    "        entries_window_k = list(itertools.islice(dataset.test_data.input, window_idx, None, n_windows))\n",
    "        forecasts_window_k = list(predictor.predict(entries_window_k))\n",
    "        forecast_windows.append(forecasts_window_k)        \n",
    "\n",
    "    forecasts = [item for items in zip(*forecast_windows) for item in items] # interleave results again\n",
    "    season_length = get_seasonality(dataset.freq)\n",
    "    return evaluate_forecasts(\n",
    "            forecasts,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        ) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    logger.info(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and ds_name not in MED_LONG_DATASETS.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        logger.info(f\"Generating forecasts for {ds_config}\")\n",
    "        all_results.append(\n",
    "            (\n",
    "                evaluate_on_dataset(\n",
    "                    model_name=model_name,\n",
    "                    ds_name=ds_name,\n",
    "                    ds_term=term,\n",
    "                    batch_size=100,\n",
    "                    use_multivariate_data=True,\n",
    "                    predict_batches_jointly=True,\n",
    "                    device_map=\"cuda\",\n",
    "                    torch_dtype=\"float32\",\n",
    "                ),\n",
    "                ds_config,\n",
    "                dataset_properties_map[ds_key][\"domain\"],\n",
    "                dataset_properties_map[ds_key][\"num_variates\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "result_df_rows = []\n",
    "for result_metrics, ds_config, domain, num_variates in all_results:\n",
    "    result_metrics = {f\"eval_metrics/{k}\": v for k, v in result_metrics[0].items()}\n",
    "\n",
    "    result_df_rows.append(\n",
    "        {\n",
    "            \"dataset\": ds_config,\n",
    "            \"model\": pretty_model_name.get(model_name, model_name),\n",
    "            **result_metrics,\n",
    "            \"domain\": domain,\n",
    "            \"num_variates\": num_variates,\n",
    "        }\n",
    "    )\n",
    "results_df = pd.DataFrame(result_df_rows).sort_values(by=\"dataset\")\n",
    "results_df.to_csv(output_dir, index=False)\n",
    "logger.info(f\"Results have been written to {output_dir}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gifteval-submission",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
