{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Run Kairos on GiftEval\n",
    "\n",
    "This notebook shows how to run [Kairos](https://github.com/foundation-model-research/Kairos) on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Optional but suggested: Install conda environment specifed in Kairos Repo**\n",
    "\n",
    "```bash\n",
    "conda create --name kairos python=3.10\n",
    "conda activate kairos\n",
    "```\n",
    "\n",
    "2. **Install Kairos**\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/foundation-model-research/Kairos.git\n",
    "cd Kairos\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "3. **Install additional dependecies needed for GiftEval benchmark**\n",
    "\n",
    "```bash\n",
    "pip install gluonts==0.14.4 dotenv datasets==3.5.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model, Forecast\n",
    "from gluonts.model.forecast import SampleForecast\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from Kairos.tsfm.model.kairos import AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"m4_weekly\"\n",
    "\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "med_long_datasets = \"bizitobs_l2c/H\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KairosPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(sequence, max_length=2048, pad_value=np.nan):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence on the left to a specified max_length.\n",
    "\n",
    "    Args:\n",
    "        sequence (list or np.ndarray): The input sequence.\n",
    "        max_length (int): The target length.\n",
    "        pad_value (int or float): The value to use for padding, defaults to np.nan.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of length max_length.\n",
    "    \"\"\"\n",
    "    seq_np = np.array(sequence)\n",
    "    current_length = len(seq_np)\n",
    "\n",
    "    if current_length < max_length:\n",
    "        # If the current length is less than the target, calculate the required padding\n",
    "        padding_size = max_length - current_length\n",
    "        # Use np.pad to add padding to the left\n",
    "        # (padding_size, 0) means pad `padding_size` elements at the beginning of the first (and only) axis\n",
    "        return np.pad(seq_np, (padding_size, 0), 'constant', constant_values=pad_value)\n",
    "    else:\n",
    "        # If the current length is greater than or equal to the target, truncate to the last max_length elements\n",
    "        return seq_np[-max_length:]\n",
    "\n",
    "class KairosPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        prediction_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(\"prediction_length:\", prediction_length)\n",
    "        self.prediction_length = prediction_length\n",
    "        # 1. Check for CUDA availability and set the primary device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # 2. Load the model\n",
    "        self.model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "        # 3. Move the model to the primary device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 256) -> List[Forecast]:\n",
    "        self.model.eval()\n",
    "        model = self.model\n",
    "        while True:\n",
    "            try:\n",
    "                # Generate forecast samples\n",
    "                forecast_outputs = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                        context = [torch.tensor(pad_or_truncate(entry[\"target\"], max_length=2048)) for entry in batch]\n",
    "                        forecast_outputs.append(\n",
    "                            model(\n",
    "                                past_target=torch.stack(context).to(self.device),\n",
    "                                prediction_length=self.prediction_length,\n",
    "                                generation=True,\n",
    "                                infer_is_positive=True,\n",
    "                                force_flip_invariance=True,\n",
    "                            )[\"prediction_outputs\"].detach().cpu().numpy()\n",
    "                        )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\n",
    "                    f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\"\n",
    "                )\n",
    "                batch_size //= 2\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "            forecasts.append(\n",
    "                SampleForecast(samples=item, start_date=forecast_start_date)\n",
    "            )\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: m4_weekly (1 of 2)\n",
      "Dataset size: 359\n",
      "prediction_length: 13\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.86it/s]\n",
      "359it [00:06, 53.83it/s]\n",
      "/tmp/ipykernel_34523/3621470440.py:129: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSE[mean]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:130: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSE[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:131: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MAE[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:132: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MASE[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:133: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MAPE[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:134: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"sMAPE[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:135: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"MSIS\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:136: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"RMSE[mean]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:137: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"NRMSE[mean]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:138: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"ND[0.5]\"][0],\n",
      "/tmp/ipykernel_34523/3621470440.py:139: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  res[\"mean_weighted_sum_quantile_loss\"][0],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_weekly have been written to ../results/Kairos_50m/all_results.csv\n",
      "Processing dataset: bizitobs_l2c/H (2 of 2)\n",
      "Dataset size: 42\n",
      "prediction_length: 48\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.22it/s]\n",
      "42it [00:00, 57.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/Kairos_50m/all_results.csv\n",
      "Dataset size: 7\n",
      "prediction_length: 480\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.06it/s]\n",
      "7it [00:00, 54.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/Kairos_50m/all_results.csv\n",
      "Dataset size: 7\n",
      "prediction_length: 720\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.35s/it]\n",
      "7it [00:00, 53.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/Kairos_50m/all_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# Model Configuration\n",
    "# model_name = \"Kairos_10m\"\n",
    "# model_path = \"mldi-lab/Kairos_10m\"\n",
    "# model_name = \"Kairos_23m\"\n",
    "# model_path = \"mldi-lab/Kairos_23m\"\n",
    "model_name = \"Kairos_50m\"\n",
    "model_path = \"mldi-lab/Kairos_50m\"\n",
    "\n",
    "# set the output directory and CSV file path\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "completed_datasets = set()\n",
    "# 1. Check if the results file exists and read the completed datasets to allow resuming\n",
    "if os.path.exists(csv_file_path):\n",
    "    print(f\"'{csv_file_path}' exists. Reading completed datasets...\")\n",
    "    with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                completed_datasets.add(row[0])\n",
    "    print(f\"Found {len(completed_datasets)} completed datasets.\")\n",
    "\n",
    "# 2. If the file doesn't exist, create it and write the header\n",
    "else:\n",
    "    with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writerow(\n",
    "            [\n",
    "                \"dataset\",\n",
    "                \"model\",\n",
    "                \"eval_metrics/MSE[mean]\",\n",
    "                \"eval_metrics/MSE[0.5]\",\n",
    "                \"eval_metrics/MAE[0.5]\",\n",
    "                \"eval_metrics/MASE[0.5]\",\n",
    "                \"eval_metrics/MAPE[0.5]\",\n",
    "                \"eval_metrics/sMAPE[0.5]\",\n",
    "                \"eval_metrics/MSIS\",\n",
    "                \"eval_metrics/RMSE[mean]\",\n",
    "                \"eval_metrics/NRMSE[mean]\",\n",
    "                \"eval_metrics/ND[0.5]\",\n",
    "                \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "                \"domain\",\n",
    "                \"num_variates\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        if ds_config in completed_datasets:\n",
    "            print(f\"Skipping already completed dataset: {ds_config}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (\n",
    "            False\n",
    "            if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "            else True\n",
    "        )\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "        predictor = KairosPredictor(\n",
    "            model_path=model_path,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "        )\n",
    "        # Measure the time taken for evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Sort by lexicographic order, frequency, and predicted length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded file.\n",
      "Data sorting complete.\n",
      "Processing complete! Results saved to '../results/Kairos_50m/all_results.csv'.\n",
      "\n",
      "Preview of the first 5 rows of the processed data:\n",
      "                 dataset       model  eval_metrics/MSE[mean]  \\\n",
      "1   bizitobs_l2c/H/short  Kairos_50m               57.918098   \n",
      "2  bizitobs_l2c/H/medium  Kairos_50m               63.733904   \n",
      "3    bizitobs_l2c/H/long  Kairos_50m               76.498069   \n",
      "0      m4_weekly/W/short  Kairos_50m           350282.823003   \n",
      "\n",
      "   eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]  \\\n",
      "1              58.555671               4.736005                0.480060   \n",
      "2              63.673946               4.684196                0.474470   \n",
      "3              76.727667               5.158500                0.556074   \n",
      "0          342247.636980             303.264569                2.431396   \n",
      "\n",
      "   eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS  \\\n",
      "1                0.451625                 0.553508           4.586803   \n",
      "2                0.485989                 0.708163           6.840211   \n",
      "3                0.603112                 0.714667          11.137332   \n",
      "0                0.076217                 0.074086          20.033768   \n",
      "\n",
      "   eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]  \\\n",
      "1                 7.610394                  0.410220              0.255283   \n",
      "2                 7.983352                  0.483404              0.283635   \n",
      "3                 8.746317                  0.534250              0.315096   \n",
      "0               591.846959                  0.107825              0.055250   \n",
      "\n",
      "   eval_metrics/mean_weighted_sum_quantile_loss        domain  num_variates  \n",
      "1                                      0.208519  Web/CloudOps             7  \n",
      "2                                      0.238645  Web/CloudOps             7  \n",
      "3                                      0.277949  Web/CloudOps             7  \n",
      "0                                      0.044315      Econ/Fin             1  \n"
     ]
    }
   ],
   "source": [
    "def process_and_sort_csv(PATH):\n",
    "    \"\"\"\n",
    "    Sort by lexicographic order, frequency, and predicted length of the dataset\n",
    "\n",
    "    Args:\n",
    "        PATH (str): The directory path containing the CSV file.\n",
    "    \"\"\"\n",
    "    input_filepath=f'{PATH}/all_results.csv'\n",
    "    output_filepath=f'{PATH}/all_results.csv'\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_filepath):\n",
    "        print(f\"Error: File not found '{input_filepath}'. Please ensure the file exists and the path is correct.\")\n",
    "        return\n",
    "\n",
    "    # 1. Load the CSV file\n",
    "    df = pd.read_csv(input_filepath)\n",
    "    print(\"Successfully loaded file.\")\n",
    "\n",
    "    # 2. Define the function for sorting\n",
    "    def get_sort_key(dataset_name):\n",
    "        \"\"\"\n",
    "        Generates a tuple for sorting based on the dataset string.\n",
    "        Sorting rules:\n",
    "        1. Before the first '/': Lexicographical order\n",
    "        2. Before the second '/': By number, then by S, T, H, D, W, M order\n",
    "        3. After the second '/': By short, medium, long order\n",
    "        \"\"\"\n",
    "        try:\n",
    "            part1, part2, part3 = dataset_name.split('/')\n",
    "        except ValueError:\n",
    "            # Handle strings that do not match the 'part1/part2/part3' format, placing them at the end\n",
    "            return (dataset_name, float('inf'), float('inf'), float('inf'))\n",
    "\n",
    "        # Define the sort mapping for the second part\n",
    "        time_unit_map = {'S': 0, 'T': 1, 'H': 2, 'D': 3, 'W': 4, 'M': 5}\n",
    "        # Use regex to separate the number and the unit\n",
    "        match = re.match(r'(\\d*)(\\w+)', part2)\n",
    "        if match:\n",
    "            num_str, unit = match.groups()\n",
    "            num = int(num_str) if num_str else 0 # Default to 0 if no number prefix\n",
    "            unit_order = time_unit_map.get(unit, float('inf'))\n",
    "        else:\n",
    "            num, unit_order = float('inf'), float('inf') # Place at the end if format doesn't match\n",
    "\n",
    "        # Define the sort mapping for the third part\n",
    "        length_map = {'short': 0, 'medium': 1, 'long': 2}\n",
    "        length_order = length_map.get(part3, float('inf')) # Place at the end if not found\n",
    "\n",
    "        # Return a tuple; Pandas will sort by the elements of the tuple in order\n",
    "        return (part1, unit_order, num, length_order)\n",
    "    \n",
    "    # 3. Apply the sorting logic\n",
    "    # Create a temporary column to store the sort keys\n",
    "    df['sort_key'] = df['dataset'].apply(get_sort_key)\n",
    "\n",
    "    # Sort by this temporary column, then drop it\n",
    "    df_sorted = df.sort_values(by='sort_key').drop(columns='sort_key')\n",
    "    print(\"Data sorting complete.\")\n",
    "\n",
    "    # 4. Save the processed data to a new file\n",
    "    df_sorted.to_csv(output_filepath, index=False)\n",
    "    print(f\"Processing complete! Results saved to '{output_filepath}'.\")\n",
    "    print(\"\\nPreview of the first 5 rows of the processed data:\")\n",
    "    print(df_sorted.head())\n",
    "\n",
    "process_and_sort_csv(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
